# Research Paper Outputs Guide

This document describes all the outputs generated by the Cutting Edge system that can be used for your research paper, including metrics, visualizations, and logs.

## Quick Start

```bash
# Run training (hyperparameter optimization)
python -m cutting_edge.main --mode train

# Run evaluation on test set  
python -m cutting_edge.main --mode eval

# Run demo with single case
python -m cutting_edge.main --mode demo
```

## Output Directory Structure

After running train and eval, you'll have the following structure:

```
cutting-edge/
├── output/                          # Main output directory
│   ├── best_config.json            # Best hyperparameters (JSON)
│   ├── training_history.json       # All training configs tested (JSON)
│   ├── evaluation_results.json     # All evaluation results (JSON)
│   │
│   ├── training_charts/            # Training visualizations
│   │   ├── training_comparison.png      # Utilization & success rate trends
│   │   ├── top_configs_comparison.png   # Top 5 configs bar chart
│   │   └── eval_time_analysis.png       # Computational efficiency
│   │
│   ├── training_logs/              # Training documentation
│   │   ├── training_metrics_YYYYMMDD_HHMMSS.csv   # CSV for Excel/tables
│   │   └── training_summary_YYYYMMDD_HHMMSS.txt   # Human-readable summary
│   │
│   ├── evaluation_charts/          # Evaluation visualizations
│   │   ├── evaluation_comprehensive.png  # 6-panel performance analysis
│   │   └── evaluation_summary.png        # Overall performance bars
│   │
│   ├── evaluation_logs/            # Evaluation documentation
│   │   ├── evaluation_metrics_YYYYMMDD_HHMMSS.csv  # CSV for Excel/tables
│   │   └── evaluation_summary_YYYYMMDD_HHMMSS.txt  # Detailed per-sample results
│   │
│   └── evaluation_images/          # Individual test case visualizations
│       ├── test_sample_001_cloth_name.png
│       ├── test_sample_002_cloth_name.png
│       └── ...
│
└── logs/                           # System logs with timestamps
    └── cutting_edge_YYYYMMDD_HHMMSS.log
```

---

## Training Outputs (--mode train)

### 1. Best Configuration (JSON)
**File**: `output/best_config.json`

**Purpose**: Contains the optimal hyperparameters found during training.

**Content**:
```json
{
  "grid_size": 25,
  "rotation_angles": [0, 90, 180, 270],
  "allow_flipping": true,
  "max_attempts": 500
}
```

**Use in Paper**: Reference these values in your Methods section.

---

### 2. Training Metrics (CSV)
**File**: `output/training_logs/training_metrics_YYYYMMDD_HHMMSS.csv`

**Purpose**: Detailed metrics for all configurations tested (for Excel/LaTeX tables).

**Columns**:
- `Config_ID`: Configuration number (1, 2, 3, 4)
- `Grid_Size`: Grid resolution (15, 20, 25)
- `Rotation_Angles`: Number of rotation angles (4 or 8)
- `Allow_Flipping`: Whether pattern flipping is enabled
- `Max_Attempts`: Maximum placement attempts
- `Train_Utilization`: Training set fabric utilization (%)
- `Train_Success_Rate`: Training set pattern placement success (%)
- `Val_Utilization`: Validation set fabric utilization (%)
- `Val_Success_Rate`: Validation set pattern placement success (%)
- `Eval_Time_Sec`: Configuration evaluation time (seconds)

**Use in Paper**: 
- Create Table 1: "Hyperparameter Search Results"
- Compare utilization across configurations
- Report best config performance

---

### 3. Training Visualizations

#### A. Training Comparison Chart
**File**: `output/training_charts/training_comparison.png`

**Content**: Two line plots showing:
- Left: Training vs. Validation Utilization
- Right: Training vs. Validation Success Rate

**Use in Paper**: Figure showing training convergence/comparison.

#### B. Top Configs Comparison
**File**: `output/training_charts/top_configs_comparison.png`

**Content**: Bar chart of top 5 configurations' utilization and success rates.

**Use in Paper**: Figure comparing best hyperparameter settings.

#### C. Evaluation Time Analysis
**File**: `output/training_charts/eval_time_analysis.png`

**Content**: Bar chart showing computational time per configuration.

**Use in Paper**: Computational efficiency analysis figure.

---

### 4. Training Summary (Text)
**File**: `output/training_logs/training_summary_YYYYMMDD_HHMMSS.txt`

**Content**: Human-readable summary with:
- Best configuration details
- Top 5 configurations ranked by validation utilization
- Evaluation times

**Use in Paper**: Reference for writing Methods and Results sections.

---

## Evaluation Outputs (--mode eval)

### 1. Evaluation Metrics (CSV)
**File**: `output/evaluation_logs/evaluation_metrics_YYYYMMDD_HHMMSS.csv`

**Purpose**: Detailed per-sample evaluation results (for Excel/LaTeX tables).

**Columns**:
- `Sample_ID`: Test sample number (1-20)
- `Cloth_File`: Cloth image filename
- `Cloth_Type`: Cloth material type
- `Cloth_Size_cm`: Cloth dimensions (width x height)
- `Num_Patterns`: Number of patterns attempted
- `Patterns_Placed`: Number of patterns successfully placed
- `Utilization_%`: Fabric utilization percentage
- `Success_Rate_%`: Pattern placement success rate
- `Waste_Area_cm2`: Wasted fabric area (cm²)
- `Processing_Time_sec`: Processing time (seconds)
- `Visualization_File`: Corresponding visualization image
- `Hole_F1`, `Stain_F1`, `Line_F1`, `Freeform_F1`: Defect detection scores
- `Grain_Error_Mean`: Average deviation from true grain direction
- `Class_Accuracy`: Pattern classification accuracy
- `Dim_MAE`: Dimension prediction Mean Absolute Error (cm)

**Use in Paper**:
- Calculate mean ± std for all metrics
- Create Table 2: "Evaluation Results on Test Set"
- Report overall performance statistics
- Analyze performance by cloth type

---

### 2. Evaluation Results (JSON)
**File**: `output/evaluation_results.json`

**Content**:
```json
{
  "timestamp": "20250106_143022",
  "summary": {
    "num_samples": 20,
    "avg_utilization": 87.34,
    "avg_success_rate": 92.15,
    "avg_waste": 245.67,
    "avg_time": 3.42,
    "total_placed": 89,
    "total_attempted": 97
  },
  "detailed_results": [...]
}
```

**Use in Paper**: Quick reference for reporting average metrics.

---

### 3. Evaluation Visualizations

#### A. Comprehensive Performance Analysis
**File**: `output/evaluation_charts/evaluation_comprehensive.png`

**Content**: 3x3 grid (9-panel) figure with:
1. Utilization distribution histogram
2. Success rate distribution histogram
3. Processing time distribution histogram
4. Train vs. Test Utilization trend
5. Train vs. Test Success Rate trend
6. Defect Detection F1 Scores (Hole, Stain, Line, Freeform)
7. Utilization vs. Pattern Count scatter
8. Per-sample Utilization bar chart
9. Per-sample Success Rate by chart

**Use in Paper**: Main results figure (Figure 3 or 4).

#### B. Performance Summary
**File**: `output/evaluation_charts/evaluation_summary.png`

**Content**: Bar chart with three key metrics:
- Average Utilization (%)
- Average Success Rate (%)
- Patterns Placed (%)

**Use in Paper**: Summary figure for abstract/introduction.

---

### 4. Individual Test Case Images
**Directory**: `output/evaluation_images/`

**Files**: `test_sample_001_cloth_name.png`, `test_sample_002_cloth_name.png`, etc.

**Content**: Visualization of each test case showing:
- Cloth outline
- Placed pattern positions
- Pattern colors/labels
- Utilization statistics

**Use in Paper**:
- Select 2-3 representative examples for qualitative analysis
- Show best case, average case, and challenging case
- Demonstrate algorithm's pattern placement quality

---

### 5. Evaluation Summary (Text)
**File**: `output/evaluation_logs/evaluation_summary_YYYYMMDD_HHMMSS.txt`

**Content**: Detailed text report with:
- Overall performance metrics
- Per-sample breakdown with all details

**Use in Paper**: Reference for writing Results section.

---

## System Logs

**File**: `logs/cutting_edge_YYYYMMDD_HHMMSS.log`

**Content**: Complete timestamped log of all operations including:
- System initialization
- Configuration applied
- Processing times
- Warnings/errors

**Use in Paper**: 
- Troubleshooting reference
- Verify experimental procedure
- Document any issues encountered

---

## Recommended Paper Structure

### Methods Section
- **Hyperparameter Optimization**: Reference `training_summary.txt` and `best_config.json`
- **Algorithm Configuration**: Report final parameters from `best_config.json`
- **Evaluation Protocol**: Describe test set from `evaluation_summary.txt`

### Results Section
- **Table 1 - Hyperparameter Search**: Use `training_metrics.csv`
- **Table 2 - Test Set Performance**: Use `evaluation_metrics.csv` 
  - Report: Mean ± Std for Utilization, Success Rate, Waste, Time
- **Figure 1 - Training Performance**: Use `training_comparison.png`
- **Figure 2 - Configuration Comparison**: Use `top_configs_comparison.png`
- **Figure 3 - Evaluation Results**: Use `evaluation_comprehensive.png`
- **Figure 4 - Example Placements**: Select 3 images from `evaluation_images/`

### Discussion Section
- **Computational Efficiency**: Reference `eval_time_analysis.png`
- **Cloth Type Analysis**: Extract from `evaluation_comprehensive.png` (waste by cloth type)

---

## Tips for LaTeX Tables

### Training Results Table
```latex
\begin{table}[h]
\centering
\caption{Hyperparameter Optimization Results}
\begin{tabular}{cccccc}
\hline
Config & Grid Size & Rotations & Val Util (\%) & Val Success (\%) & Time (s) \\
\hline
1 & 20 & 4 & 85.2 & 90.1 & 2.34 \\
2 & 20 & 8 & 87.4 & 92.3 & 3.12 \\
3 & 25 & 4 & 86.1 & 91.0 & 2.89 \\
\textbf{4} & \textbf{25} & \textbf{8} & \textbf{88.9} & \textbf{93.5} & \textbf{3.45} \\
\hline
\end{tabular}
\label{tab:hyperparams}
\end{table}
```

### Evaluation Results Table
```latex
\begin{table}[h]
\centering
\caption{Test Set Evaluation Results}
\begin{tabular}{lc}
\hline
Metric & Value \\
\hline
Fabric Utilization (\%) & 87.34 ± 4.21 \\
Pattern Success Rate (\%) & 92.15 ± 3.67 \\
Waste Area (cm²) & 245.67 ± 89.34 \\
Processing Time (s) & 3.42 ± 0.87 \\
Samples Evaluated & 20 \\
\hline
\end{tabular}
\label{tab:eval_results}
\end{table}
```

---

## Calculating Statistics from CSV

### In Excel
1. Open `evaluation_metrics_*.csv`
2. Select `Utilization_%` column
3. Use formulas:
   - Mean: `=AVERAGE(G2:G21)`
   - Std Dev: `=STDEV.S(G2:G21)`
4. Repeat for other metrics

### In Python
```python
import pandas as pd

# Load evaluation data
df = pd.read_csv('output/evaluation_logs/evaluation_metrics_*.csv')

# Calculate statistics
print(f"Utilization: {df['Utilization_%'].mean():.2f} ± {df['Utilization_%'].std():.2f}%")
print(f"Success Rate: {df['Success_Rate_%'].mean():.2f} ± {df['Success_Rate_%'].std():.2f}%")
print(f"Waste: {df['Waste_Area_cm2'].mean():.2f} ± {df['Waste_Area_cm2'].std():.2f} cm²")
print(f"Time: {df['Processing_Time_sec'].mean():.3f} ± {df['Processing_Time_sec'].std():.3f}s")
```

---

## Key Metrics for Your Paper

### Primary Metrics
1. **Fabric Utilization (%)**: Higher is better (goal: >85%)
2. **Pattern Placement Success Rate (%)**: Higher is better (goal: >90%)
3. **Waste Area (cm²)**: Lower is better
4. **Processing Time (s)**: Lower is better

### Secondary Metrics
5. **Patterns Placed / Total**: Overall success ratio
6. **Performance by Cloth Type**: Compare COTTON, DENIM, SILK, etc.

---

## Questions for Your Paper

All outputs are designed to help answer:

1. **Does hyperparameter optimization improve performance?**
   → Compare training configs in `training_metrics.csv`

2. **What are the optimal parameters?**
   → Reference `best_config.json`

3. **How well does the system perform?**
   → Report metrics from `evaluation_metrics.csv`

4. **Is performance consistent?**
   → Show distribution histograms from `evaluation_comprehensive.png`

5. **Does it work on different cloth types?**
   → Analyze waste by cloth type from `evaluation_comprehensive.png`

6. **Is it computationally efficient?**
   → Report processing times from CSV and `eval_time_analysis.png`

---

## Need Help?

If any outputs are missing or unclear:
1. Check `logs/cutting_edge_*.log` for error messages
2. Verify the commands completed successfully
3. Re-run with `--mode train` or `--mode eval`

All files are timestamped, so you can run multiple experiments and compare results.
